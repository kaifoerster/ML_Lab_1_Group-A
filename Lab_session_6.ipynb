{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaifoerster/ML_Lab_1_Group-A/blob/main/Lab_session_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY8CkEADXQkA"
      },
      "source": [
        "#Lab Session 6\n",
        "\n",
        "We will implement a RNN from scratch using `pytorch`, based on Chapter 9 and 10 of the textbook (<a href=\"https://d2l.ai/chapter_recurrent-neural-networks/rnn.html\">Zhang et al.</a>), and use it to implement a simple language model.\n",
        "\n",
        "Note well: Make sure that external cookies are enabled for this notebook to be able to run all cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_9P4yxuljby_",
        "outputId": "b54cd99c-bea8-4db6-8e52-101fa6325926"
      },
      "outputs": [],
      "source": [
        "#!pip install torch==2.0.0 torchvision==0.15.1\n",
        "#!pip install d2l==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Efgpp5MYg5Tl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import re\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imzoXqSTY0MB"
      },
      "source": [
        "##Preprocessing sequential data\n",
        "\n",
        "We will working with natural language sentences, which by definition, are sequential data.\n",
        "\n",
        "**Definition**: Sequential Data is characterized by: i) that the ordering of instances is relevant and ii) that instances depend on other instances in the dataset.\n",
        "\n",
        "An important part of dealing with natural language sequences is defining the input units for the algorithms (tokens) and translate it to numerical input. Each time step will correspond to 1 token, but what precisely constitutes a token is a design choice:\n",
        "<ul>\n",
        "<li>If we define tokens to be words, then\n",
        "in the sequence \"Mary has a little lamb\", $x_1$ would be \"Mary\"</li>\n",
        "<li>If we define them to be letters, it would $x_1$ would be \"M\"</li>\n",
        "</ul>\n",
        "\n",
        "We then encode tokens by assigning to each different token in our sequence an integer representing its position in the comprenshive list of all the tokens in our dataframe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HViVEYn_vIL_"
      },
      "source": [
        "#Loading the data\n",
        "\n",
        "We will be working with H. G. Wellsâ€™ The Time Machine. For simplicity, we will define tokens to be single letters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oVapguFdOQjG"
      },
      "outputs": [],
      "source": [
        "class TimeMachine(d2l.DataModule):\n",
        "    \"\"\"The Time Machine dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
        "\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        corpus, self.vocab = self.build(self._download())\n",
        "        array = d2l.tensor([corpus[i:i+num_steps+1]\n",
        "                            for i in range(len(corpus)-num_steps)])\n",
        "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
        "\n",
        "    def build(self, raw_text, vocab=None):\n",
        "\n",
        "        tokens = self._tokenize(self._preprocess(raw_text))\n",
        "        if vocab is None: vocab = d2l.Vocab(tokens)\n",
        "        corpus = [vocab[token] for token in tokens]\n",
        "        return corpus, vocab\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        idx = slice(0, self.num_train) if train else slice(\n",
        "            self.num_train, self.num_train + self.num_val)\n",
        "        return self.get_tensorloader([self.X, self.Y], train, idx)\n",
        "\n",
        "    def _download(self):\n",
        "        fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,\n",
        "                             '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
        "        with open(fname) as f:\n",
        "            return f.read()\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "      #TODO: implement the tokenization step\n",
        "      return list(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0kSsxwzJ_qo"
      },
      "source": [
        "Task: implement the tokenization step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "iNQ8JeSxvdrd"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def _tokenize(self, text):\n",
        "  return list(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M6fSPHlnaEyO"
      },
      "outputs": [],
      "source": [
        "data = TimeMachine(batch_size=2**10, num_steps=2**5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG83rMgkchA6"
      },
      "source": [
        "##Implementing a vanilla RNN\n",
        "We define a vanilla RNN with three parameters:\n",
        "\n",
        "- $W_{x}$: the weight matrix multiplying input $X_t$\n",
        "- $W_{h}$: the weight matrix multiplying hidden state $H_{t-1}$\n",
        "- $b$: the bias of the hidden state.\n",
        "\n",
        "Altogether, the class applies the following recursion:\n",
        "\n",
        "$f(X_t) = \\phi(W_x X_t + W_h H_{t-1}+b) =  \\phi\\bigg(W_x X_t + W_h \\phi(W_x X_{t-1} + W_h H_{t-2} )+b\\bigg) = \\dots $\n",
        "\n",
        "Can you guess what will be $f(X_1)$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GfplYL6uwARu"
      },
      "outputs": [],
      "source": [
        "class RNN(d2l.Module):\n",
        "\n",
        "    \"\"\"The RNN model implemented from scratch.\"\"\"\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Weight matrix that multiplies input X_t\n",
        "        self.W_x = nn.Parameter(\n",
        "            torch.randn(num_inputs, num_hiddens) * sigma)\n",
        "\n",
        "        # Weight matrix that multiplies output of hidden layer H_(t-1)\n",
        "        self.W_h = nn.Parameter(\n",
        "            torch.randn(num_hiddens, num_hiddens) * sigma)\n",
        "\n",
        "        #Bias of the hidden layer\n",
        "        self.b = nn.Parameter(torch.zeros(num_hiddens))\n",
        "\n",
        "        #Activation function of the hidden layer\n",
        "        self.activation_func = torch.tanh\n",
        "\n",
        "    def forward(self, inputs, state=None):\n",
        "      \"\"\"Executes the RNN recurrent step.\n",
        "      Inputs shape: (num_steps, batch_size, num_inputs)\"\"\"\n",
        "\n",
        "      outputs = []\n",
        "\n",
        "      if state is None:\n",
        "          # Initial state with shape: (batch_size, num_hiddens)\n",
        "          state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                            device=inputs.device)\n",
        "      else:\n",
        "        #Transformed input list in variable\n",
        "          state, = state\n",
        "\n",
        "      for X in inputs:\n",
        "          state = self.activation_func(torch.matmul(X, self.W_x) +\n",
        "                          torch.matmul(state, self.W_h) + self.b)\n",
        "          outputs.append(state)\n",
        "\n",
        "      return outputs, state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUCic-nd-fO2"
      },
      "source": [
        "##Implementing a classifier using RNNs\n",
        "\n",
        "We will use our RNN to predict the next $n$ tokens in a sequence of characters.\n",
        "This problem is a classification problem because our input sequence is composed of integer tokens from a finite sets (alphanumeric characters). Hence we will use inherite from `d2l.Classifier` appropriate loss and accuracy definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PimvbrAnfrN6"
      },
      "outputs": [],
      "source": [
        "class RNNLModel(d2l.Classifier):\n",
        "\n",
        "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.W_h_out = nn.Parameter(\n",
        "            d2l.randn(\n",
        "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
        "        self.b_out = nn.Parameter(d2l.zeros(self.vocab_size))\n",
        "# implicit application of softmax\n",
        "\n",
        "    def one_hot(self, X):\n",
        "        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "\n",
        "    def forward(self, X, state=None):\n",
        "\n",
        "        #encode input as binary\n",
        "        embs = self.one_hot(X)\n",
        "        #apply recurrent step\n",
        "        rnn_outputs, _ = self.rnn(embs, state)\n",
        "        #apply weight matrix on all hidden state\n",
        "        #Since we are using torch.nn.CrossEntropyLoss, no need for softmax output func\n",
        "        outputs = [d2l.matmul(H, self.W_h_out) + self.b_out for H in rnn_outputs]\n",
        "        outputs = d2l.stack(outputs, 1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        self.plot('ppl', d2l.exp(l), train=True)\n",
        "        return l\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        self.plot('ppl', d2l.exp(l), train=False)\n",
        "\n",
        "def predict(self, prefix, num_preds, vocab, device=None):\n",
        "    device = device or self.W_h_out.device\n",
        "    state, outputs = None, [vocab[prefix[0]]]\n",
        "    for i in range(len(prefix) + num_preds - 1):\n",
        "        X = d2l.tensor([[outputs[-1]]], device=device)\n",
        "        embs = self.one_hot(X)\n",
        "        rnn_outputs, state = self.rnn(embs, state)\n",
        "        if i < len(prefix) - 1:  # Warm-up period\n",
        "            outputs.append(vocab[prefix[i + 1]])\n",
        "        else:  # Predict num_preds steps\n",
        "            Y = d2l.matmul(rnn_outputs, self.W_h_out) + self.b_out  # this line was modified\n",
        "            outputs.append(int(d2l.reshape(d2l.argmax(Y, axis=2), 1)))\n",
        "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lBm9t0_oNU7c"
      },
      "outputs": [],
      "source": [
        "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
        "rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)\n",
        "model = RNNLModel(rnn, len(data.vocab))\n",
        "#trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\n",
        "#trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'RNNLModel' object has no attribute 'predict'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\kaius\\OneDrive - Hertie School\\MSc Data Science for Public Policy\\Semester_3\\Deep Learning\\ML_Lab_1_Group-A\\Lab_session_6.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kaius/OneDrive%20-%20Hertie%20School/MSc%20Data%20Science%20for%20Public%20Policy/Semester_3/Deep%20Learning/ML_Lab_1_Group-A/Lab_session_6.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#trainer.predict(\"let me try to make a prediction\", 10, data.vocab)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kaius/OneDrive%20-%20Hertie%20School/MSc%20Data%20Science%20for%20Public%20Policy/Semester_3/Deep%20Learning/ML_Lab_1_Group-A/Lab_session_6.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39m\u001b[39mi am\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, data\u001b[39m.\u001b[39mvocab)\n",
            "File \u001b[1;32mc:\\Users\\kaius\\.virtualenvs\\ML_Lab_1_Group-A-YrkWYMVn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'RNNLModel' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "def predict(self, prefix, num_preds, vocab, device=None):\n",
        "    device = device or self.W_h_out.device\n",
        "    state, outputs = None, [vocab[prefix[0]]]\n",
        "    for i in range(len(prefix) + num_preds - 1):\n",
        "        X = d2l.tensor([[outputs[-1]]], device=device)\n",
        "        embs = self.one_hot(X)\n",
        "        rnn_outputs, state = self.rnn(embs, state)\n",
        "        if i < len(prefix) - 1:  # Warm-up period\n",
        "            outputs.append(vocab[prefix[i + 1]])\n",
        "        else:  # Predict num_preds steps\n",
        "            Y = d2l.matmul(rnn_outputs, self.W_h_out) + self.b_out  # this line was modified\n",
        "            outputs.append(int(d2l.reshape(d2l.argmax(Y, axis=2), 1)))\n",
        "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuARk9YZwoaR"
      },
      "source": [
        "##Bonus: implementing a classifier using a LSTM structure\n",
        "\n",
        "We now implement a more sophisticated classifier using LSTM. In a LSTM network, each memory cell is equipped with an internal state $C$ and other gates that determine:\n",
        "<ul>\n",
        "<li>the impact of input on the internal state $C$ (input gate $I$)</li>\n",
        "<li>the internal state should be flushed to (forget gate $F$) </li>\n",
        "<li>the impact of the internal state on the the output of the cell the output gate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz_dmglF-ux7"
      },
      "outputs": [],
      "source": [
        "class LSTM(d2l.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
        "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
        "                          init_weight(num_hiddens, num_hiddens),\n",
        "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
        "\n",
        "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
        "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
        "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
        "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "\n",
        "        if H_C is None:\n",
        "            # Initial state with shape: (batch_size, num_hiddens)\n",
        "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                          device=inputs.device)\n",
        "            C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                          device=inputs.device)\n",
        "        else:\n",
        "            H, C = H_C\n",
        "\n",
        "        outputs = []\n",
        "        for X in inputs:\n",
        "            #sigmoid: output between [0,1]\n",
        "            I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
        "                            torch.matmul(H, self.W_hi) + self.b_i)\n",
        "            #sigmoid: output between [0,1]\n",
        "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
        "                            torch.matmul(H, self.W_hf) + self.b_f)\n",
        "            #sigmoid: output between [0,1]\n",
        "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
        "                            torch.matmul(H, self.W_ho) + self.b_o)\n",
        "            #tanh: output between [-1,1].\n",
        "            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
        "                              torch.matmul(H, self.W_hc) + self.b_c)\n",
        "\n",
        "            #update internal state\n",
        "            C = F * C + I * C_tilde\n",
        "            #output of the hidden cell is recurrent output O times activated internal state C\n",
        "            H = O * torch.tanh(C)\n",
        "\n",
        "            outputs.append(H)\n",
        "        return outputs, (H, C)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQO153iZF534"
      },
      "source": [
        "Question: what happens if $W_{xc}$,$W_{hc}$ and $b_c$ are zero matrices?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ww0Ne3LZHSW-"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "C = F * C = 0   #network has no memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTP4lZXbGlAS"
      },
      "source": [
        "Question: what is the value of the update of $C$ on the first loop (i.e. for $X_1$)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZlWAPrwHJET"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "C = I * C_tilde #cell does not remember anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfgzUm55H4nI"
      },
      "outputs": [],
      "source": [
        "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
        "lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)\n",
        "model = RNNLModel(lstm, vocab_size=len(data.vocab), lr=4)\n",
        "trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
